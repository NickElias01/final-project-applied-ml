{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c24776",
   "metadata": {},
   "source": [
    "# Applied Machine Learning - Housing Price Analysis\n",
    "### Nick Elias, Elias Analytics\n",
    "### Date: 4/20/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242325b5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project focuses on predicting housing prices (SalePrice) using machine learning techniques applied to a real estate dataset \"train.csv\", found at [Kaggle Housing Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). The goal is to build accurate regression models through effective data preprocessing, feature engineering, and model evaluation. This analysis could be applied to the \"test.csv\" dataset for Kaggle competition entry.\n",
    "\n",
    "Key features selected for their strong correlation with housing prices include:\n",
    "- `GrLivArea` (Above ground living area)\n",
    "- `HouseAge` (Age of the house)\n",
    "- `OverallQual` (Overall quality score)\n",
    "\n",
    "Three modeling pipelines were developed:\n",
    "\n",
    "1. **Baseline Pipeline**  \n",
    "   - Applies standard scaling  \n",
    "   - Uses linear regression\n",
    "\n",
    "2. **Pipeline with Imputation**  \n",
    "   - Adds missing value imputation  \n",
    "   - Enhances robustness and data quality\n",
    "\n",
    "3. **Advanced Polynomial Pipeline**  \n",
    "   - Incorporates polynomial features (e.g., degree 3)  \n",
    "   - Captures non-linear relationships in the data\n",
    "\n",
    "\n",
    "Model performance was assessed using:\n",
    "- **RMSE** (Root Mean Squared Error)\n",
    "- **R²** (Coefficient of Determination)\n",
    "\n",
    "Additional techniques included:\n",
    "- **Cross-validation** for measuring generalization across different data splits\n",
    "- **Learning curves** to evaluate overfitting and underfitting tendencies\n",
    "\n",
    "### Best Performing Model\n",
    "The **Advanced Polynomial Pipeline** delivered the strongest performance, demonstrating the value of:\n",
    "- Feature scaling\n",
    "- Non-linear feature expansion\n",
    "- Thorough preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn: Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Scikit-learn: Model selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "\n",
    "# Scikit-learn: Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier  # Uncomment if needed for classification\n",
    "# from sklearn.tree import DecisionTreeClassifier, plot_tree  # Uncomment if needed for classification\n",
    "\n",
    "# Scikit-learn: Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # Uncomment for classification\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report  # Uncomment for classification\n",
    "\n",
    "# Other utilities\n",
    "from itertools import combinations\n",
    "\n",
    "# Custom preprocessing pipeline (if applicable)\n",
    "from preprocessing_pipeline import preprocess_data, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0105809",
   "metadata": {},
   "source": [
    "## Section 1. Import and Inspect the Data\n",
    "### 1.1 Load the dataset and display the first 10 rows.\n",
    "### 1.2 Check for missing values and display summary statistics.\n",
    "### Reflection 1: What do you notice about the dataset? Are there any data issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acab14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Display all columns in the DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(\"First 10 rows of the dataset:\")\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c68869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns are displayed without truncation\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)  # Adjusts the width to avoid wrapping\n",
    "pd.set_option('display.max_colwidth', None)  # Ensures full content of each column is shown\n",
    "\n",
    "# Display column names and their data types\n",
    "print(\"Column names and their data types:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns with missing data\n",
    "missing_data = data.isnull().sum()\n",
    "missing_columns = missing_data[missing_data > 0]\n",
    "print(\"Columns with missing data:\")\n",
    "print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\nSummary statistics of the dataset:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8306a79",
   "metadata": {},
   "source": [
    "### **Dataset Observations:**\n",
    "\n",
    "The dataset contains real estate data related to housing prices.\n",
    "\n",
    "*General Structure:*\n",
    "\n",
    "- The dataset contains a mix of numerical and categorical columns.\n",
    "- The target variable seems to be SalePrice, which represents the price of a house.\n",
    "\n",
    "*Columns with Missing Data:*\n",
    "\n",
    "- Some columns, such as Alley, FireplaceQu, PoolQC, Fence, and MiscFeature, have missing values (NaN).\n",
    "- These columns may require imputation or removal depending on their importance.\n",
    "\n",
    "*Features:*\n",
    "\n",
    "- Numerical Features: Examples include LotFrontage, LotArea, YearBuilt, GrLivArea, and SalePrice.\n",
    "- Categorical Features: Examples include MSZoning, Street, LotShape, and Neighborhood.\n",
    "\n",
    "*Potential Relationships:*\n",
    "\n",
    "- Features like GrLivArea, OverallQual, and YearBuilt might have a strong correlation with SalePrice.\n",
    "- Categorical features like Neighborhood and MSZoning could also influence housing prices.\n",
    "\n",
    "*Outliers and Anomalies:*\n",
    "\n",
    "- Some columns, such as LotFrontage, have missing values in certain rows.\n",
    "- Columns like GarageYrBlt have numerical values but may need special handling for missing data.\n",
    "\n",
    "*Target Variable:*\n",
    "\n",
    "- SalePrice is the target variable for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ffd89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f3b6c",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration and Preparation\n",
    "### 2.1 Explore data patterns and distributions\n",
    "- Create histograms, boxplots, and count plots for categorical variables (as applicable).\n",
    "- Identify patterns, outliers, and anomalies in feature distributions.\n",
    "- Check for class imbalance in the target variable (as applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for MSSubClass\n",
    "mssubclass_mapping = {\n",
    "    20: \"1-STORY 1946 & NEWER\",\n",
    "    30: \"1-STORY 1945 & OLDER\",\n",
    "    40: \"1-STORY W/FINISHED ATTIC\",\n",
    "    45: \"1-1/2 STORY - UNFINISHED\",\n",
    "    50: \"1-1/2 STORY FINISHED\",\n",
    "    60: \"2-STORY 1946 & NEWER\",\n",
    "    70: \"2-STORY 1945 & OLDER\",\n",
    "    75: \"2-1/2 STORY ALL AGES\",\n",
    "    80: \"SPLIT OR MULTI-LEVEL\",\n",
    "    85: \"SPLIT FOYER\",\n",
    "    90: \"DUPLEX - ALL STYLES\",\n",
    "    120: \"1-STORY PUD 1946 & NEWER\",\n",
    "    150: \"1-1/2 STORY PUD\",\n",
    "    160: \"2-STORY PUD 1946 & NEWER\",\n",
    "    180: \"PUD - MULTILEVEL\",\n",
    "    190: \"2 FAMILY CONVERSION\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to the MSSubClass column\n",
    "data['MSSubClassMapped'] = data['MSSubClass'].map(mssubclass_mapping)\n",
    "\n",
    "# Display unique values in MSSubClass and their mapped names\n",
    "print(\"Unique values in MSSubClass:\")\n",
    "print(data['MSSubClass'].unique())\n",
    "\n",
    "print(\"\\nUnique mapped values in MSSubClassMapped:\")\n",
    "print(data['MSSubClassMapped'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots for numerical variables\n",
    "numerical_vars = ['GrLivArea', 'YearBuilt']\n",
    "for var in numerical_vars:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=var, y='SalePrice', data=data)\n",
    "    plt.title(f'{var} vs SalePrice')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for categorical variables\n",
    "categorical_vars = ['MSSubClassMapped', 'Neighborhood', 'GarageType','OverallQual']\n",
    "for var in categorical_vars:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=var, y='SalePrice', data=data)\n",
    "    plt.title(f'{var} vs SalePrice')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff047b7b",
   "metadata": {},
   "source": [
    "### *Patterns, outliers, and anomalies*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ae40e",
   "metadata": {},
   "source": [
    "### Observations on Variables\n",
    "\n",
    "**'GrLivArea'**\n",
    "- **Pattern:** Positive linear relationship with `SalePrice`.\n",
    "- **Outliers:**\n",
    "    - Only 4 listings exceed 4000 sqft.\n",
    "    - The largest listing (over 4000 sqft) is surprisingly cheap, priced under $200k.\n",
    "- **Anomalies:** None identified.\n",
    "\n",
    "**'OverallQual'**\n",
    "- **Pattern:** Positive linear relationship; better quality correlates with higher `SalePrice`.\n",
    "- **Outliers:**\n",
    "    - Quality score 4 has one outlier priced about $75k higher than other listings.\n",
    "- **Anomalies:** Higher quality scores exhibit greater variance. A box plot may help visualize this.\n",
    "\n",
    "**'YearBuilt'**\n",
    "- **Pattern:** No obvious correlation with `SalePrice`.\n",
    "- **Outliers:**\n",
    "    - A house built around 1890 is over $100k more expensive than the next most expensive.\n",
    "    - Some houses built in the 1990s are at least $200k more expensive than the next.\n",
    "- **Anomalies:** Unclear.\n",
    "\n",
    "**'MSSubClass'**\n",
    "- **Pattern:**\n",
    "    - \"2-Story 1946 & Newer\" has the highest maximum price and average price but also many outliers.\n",
    "    - \"1-Story PUD 1946 & Newer\" has the lowest average and minimum prices.\n",
    "    - Median distributions show little variance, suggesting it may not be a strong predictor.\n",
    "- **Outliers:**\n",
    "    - \"2-Story 1946 & Newer\" and \"1-Story 1946 & Newer\" have outliers above the plot.\n",
    "- **Anomalies:** Unclear.\n",
    "\n",
    "**'Neighborhood'**\n",
    "- **Pattern:** Significant variance in median `SalePrice`, indicating it might be a strong predictor.\n",
    "- **Outliers:**\n",
    "    - \"NoRidge\" has an average around $300k but outliers peak over $700k.\n",
    "- **Anomalies:** Unclear.\n",
    "\n",
    "**'GarageType'**\n",
    "- **Pattern:** Some variance in median `SalePrice` across categories.\n",
    "- **Outliers:**\n",
    "    - \"Attachd\" has many outliers above the plot.\n",
    "- **Anomalies:** Unclear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00384dbe",
   "metadata": {},
   "source": [
    "#### Class Inbalance Check\n",
    "First, checking the distribution of Target Variable SalePrice\n",
    "\n",
    "Then, checking distibution of 6 chosen attributes. Optional, but will benefit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d77b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram for the target variable\n",
    "data['SalePrice'].hist(bins=20, figsize=(8, 6))\n",
    "plt.title(\"Distribution of SalePrice\")\n",
    "plt.xlabel(\"SalePrice\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780a296",
   "metadata": {},
   "source": [
    "- The distribution of SalePrice is right-skewed (positively skewed), meaning most of the house prices are concentrated in the lower range (e.g., below $200,000), while fewer houses are in the higher price ranges.\n",
    "\n",
    "- There is a long tail extending toward higher prices, indicating a small number of expensive houses.\n",
    "\n",
    "- Models might struggle to predict higher-priced houses accurately because they are underrepresented in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Check class distribution for categorical variables\n",
    "categorical_vars = ['MSSubClassMapped', 'Neighborhood', 'GarageType']\n",
    "for var in categorical_vars:\n",
    "    print(f\"Class distribution for {var}:\")\n",
    "    print(data[var].value_counts(normalize=True) * 100)  # Display percentages\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416014d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Plot histograms for numerical variables\n",
    "#numerical_vars = ['GrLivArea', 'OverallQual', 'YearBuilt']\n",
    "#for var in numerical_vars:\n",
    "#    data[var].hist(bins=20, figsize=(8, 6))\n",
    "#    plt.title(f\"Distribution of {var}\")\n",
    "#    plt.xlabel(var)\n",
    "#    plt.ylabel(\"Frequency\")\n",
    "#    plt.grid(alpha=0.3)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e69fb",
   "metadata": {},
   "source": [
    "### 2.2 Handle missing values and clean data\n",
    "- Impute or drop missing values (as applicable).\n",
    "- Remove or transform outliers (as applicable).\n",
    "- Convert categorical data to numerical format using encoding (as applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69faafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 6 interesting variables\n",
    "chosen_vars = ['GrLivArea', 'OverallQual', 'YearBuilt', 'MSSubClassMapped', 'Neighborhood', 'GarageType']\n",
    "\n",
    "# Check for missing values in the chosen variables\n",
    "missing_values = data[chosen_vars].isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "total_records = len(data)\n",
    "missing_percentage = (missing_values / total_records) * 100\n",
    "\n",
    "# Combine the counts and percentages into a DataFrame for better readability\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "print(\"Missing values in the chosen variables:\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57997832",
   "metadata": {},
   "source": [
    "There are clear visual outliers in the target variable (houses over $500k) and in the GrLivArea variable (values over 4000). Before we make changes, let's see how many records would be impacted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5891410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of records in the dataset\n",
    "total_records = len(data)\n",
    "\n",
    "# Count values where SalePrice > 500k\n",
    "saleprice_above_500k = data[data['SalePrice'] > 500000]\n",
    "count_saleprice_above_500k = len(saleprice_above_500k)\n",
    "percentage_saleprice_above_500k = (count_saleprice_above_500k / total_records) * 100\n",
    "\n",
    "# Count values where GrLivArea > 4000\n",
    "grlivarea_above_4000 = data[data['GrLivArea'] > 4000]\n",
    "count_grlivarea_above_4000 = len(grlivarea_above_4000)\n",
    "percentage_grlivarea_above_4000 = (count_grlivarea_above_4000 / total_records) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of houses with SalePrice > 500k: {count_saleprice_above_500k} ({percentage_saleprice_above_500k:.2f}%)\")\n",
    "print(f\"Number of houses with GrLivArea > 4000: {count_grlivarea_above_4000} ({percentage_grlivarea_above_4000:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e22cc",
   "metadata": {},
   "source": [
    "These outlier records represent a very small portion of the total data. Let's remove them to reduce skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers where SalePrice > 500k or GrLivArea > 4000\n",
    "data_cleaned = data[(data['SalePrice'] <= 500000) & (data['GrLivArea'] <= 4000)]\n",
    "\n",
    "# Calculate the number of records removed\n",
    "records_removed = len(data) - len(data_cleaned)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original dataset size: {len(data)}\")\n",
    "print(f\"Cleaned dataset size: {len(data_cleaned)}\")\n",
    "print(f\"Number of records removed: {records_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67f480",
   "metadata": {},
   "source": [
    "### 2.3 Feature selection and engineering\n",
    "- Create new features (as applicable).\n",
    "- Transform or combine existing features to improve model performance (as applicable).\n",
    "- Scale or normalize data (as applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HouseAge feature using .loc\n",
    "data_cleaned.loc[:, 'HouseAge'] = 2025 - data_cleaned['YearBuilt']\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(data_cleaned[['YearBuilt', 'HouseAge']].head())\n",
    "\n",
    "# Scatterplot of HouseAge vs SalePrice\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='HouseAge', y='SalePrice', data=data_cleaned, color='blue', alpha=0.6)\n",
    "plt.title(\"HouseAge vs SalePrice\", fontsize=16)\n",
    "plt.xlabel(\"HouseAge (years)\", fontsize=12)\n",
    "plt.ylabel(\"SalePrice\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of the HouseAge variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "data_cleaned['HouseAge'].hist(bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution of HouseAge\", fontsize=16)\n",
    "plt.xlabel(\"HouseAge (years)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "numerical_features = ['GrLivArea', 'HouseAge']\n",
    "categorical_features_onehot = ['Neighborhood','MSSubClassMapped']  # Use OneHotEncoding for these\n",
    "categorical_features_ordinal = ['OverallQual']  # Use OrdinalEncoding for these\n",
    "\n",
    "# Define transformers\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "ordinal = OrdinalEncoder()\n",
    "\n",
    "# Combine transformations using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, numerical_features),  # Scale numerical features\n",
    "        ('onehot', ohe, categorical_features_onehot),  # OneHotEncode selected categorical features\n",
    "        ('ordinal', ordinal, categorical_features_ordinal)  # OrdinalEncode selected categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "data_transformed = preprocessor.fit_transform(data_cleaned)\n",
    "\n",
    "# Get feature names for the transformed data\n",
    "numerical_feature_names = numerical_features\n",
    "onehot_feature_names = preprocessor.named_transformers_['onehot'].get_feature_names_out(categorical_features_onehot)\n",
    "ordinal_feature_names = categorical_features_ordinal\n",
    "all_feature_names = list(numerical_feature_names) + list(onehot_feature_names) + list(ordinal_feature_names)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "data_transformed_df = pd.DataFrame(data_transformed, columns=all_feature_names, index=data_cleaned.index)\n",
    "\n",
    "# Display the first few rows of the transformed data\n",
    "print(data_transformed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3f7a9",
   "metadata": {},
   "source": [
    "### Reflection 2: What patterns or anomalies do you see? Do any features stand out? What preprocessing steps were necessary to clean and improve the data? Did you create or modify any features to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe7f3b",
   "metadata": {},
   "source": [
    "Outliers were removed in the following variables:\n",
    "\n",
    "- GrLivArea: records > 4000 sqft were priced significantly lower than expected, so they were removed.\n",
    "- SalePrice (target variable): records > $500k were found to be outliers and removed from analysis.\n",
    "\n",
    "Features that stands out: \n",
    "\n",
    "- OverallQual has what appears to be a strong positive correlation with SalePrice; higher quality scores generally correspond to higher prices.\n",
    "- Neighborhood has significant variance in median SalePrice, suggesting it might be a strong categorical predictor as well\n",
    "\n",
    "Preprocessing steps:\n",
    "\n",
    "- Created HouseAge to replace YearBuilt. See explanation in following cell.\n",
    "- Encoded categorical variables for compatibility with machine learning models.\n",
    "- Used StandardScaler to standardize numerical features for better fit with machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944541cc",
   "metadata": {},
   "source": [
    "### *Why Create HouseAge Instead of Using YearBuilt?*\n",
    "Creating HouseAge has several advantages over directly using YearBuilt:\n",
    "\n",
    "- Interpretability:\n",
    "\n",
    "    - HouseAge is easier to interpret because it directly represents how old a house is, which is often more meaningful than the year it was built.\n",
    "\n",
    "- Linear Relationships:\n",
    "\n",
    "    - HouseAge may have a more linear relationship with the target variable (SalePrice) compared to YearBuilt, which is often non-linear.\n",
    "\n",
    "- Temporal Relevance:\n",
    "\n",
    "    - YearBuilt is tied to a specific calendar year, which may lose relevance over time. HouseAge dynamically adjusts as the current year changes, making it more robust for future predictions.\n",
    "\n",
    "- Feature Scaling:\n",
    "\n",
    "    - YearBuilt has a large range (e.g., 1800–2025), which can make it harder to scale or normalize. HouseAge typically has a smaller range (e.g., 0–200), making it easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0636279",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0f3a8",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "### 3.1 Choose features and target\n",
    "- Select two or more input features (numerical for regression, numerical and/or categorical for classification)\n",
    "- Select a target variable (as applicable)\n",
    "- Regression: Continuous target variable (e.g., price, temperature).\n",
    "- Classification: Categorical target variable (e.g., gender, species).\n",
    "- Clustering: No target variable.\n",
    "- Justify your selection with reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9887c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# Define Target and Input Features for Regression Analysis\n",
    "##############################################################################################\n",
    "\n",
    "# Target feature\n",
    "target_feature = 'SalePrice'  # Numerical (continuous)\n",
    "\n",
    "# Input features\n",
    "input_features = {\n",
    "    'GrLivArea': 'Numerical',\n",
    "    'HouseAge': 'Numerical',\n",
    "    'OverallQual': 'Categorical (Ordinal)',\n",
    "    'MSSubClassMapped': 'Categorical (Nominal)',\n",
    "    'Neighborhood': 'Categorical (Nominal)'\n",
    "}\n",
    "\n",
    "# Display the target and input features\n",
    "print(\"### Regression Analysis ###\")\n",
    "print(f\"Target Feature: {target_feature} (Numerical - Continuous)\")\n",
    "print(\"\\nInput Features:\")\n",
    "for feature, feature_type in input_features.items():\n",
    "    print(f\"- {feature}: {feature_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb121546",
   "metadata": {},
   "source": [
    "### 3.2 Define X and y\n",
    "- Assign input features to X\n",
    "- Assign target variable to y (as applicable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable (y) for Linear Regression\n",
    "y_linear = data_cleaned['SalePrice']\n",
    "\n",
    "# Define the input features (X) for Linear Regression (exclude Neighborhood and MSSubClassMapped)\n",
    "X_linear = data_cleaned[['GrLivArea', 'HouseAge', 'OverallQual']]\n",
    "\n",
    "\n",
    "# Display the shapes of X and y to confirm\n",
    "print(f\"Shape of X_linear: {X_linear.shape}\")\n",
    "print(f\"Shape of y_linear: {y_linear.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6251b8a",
   "metadata": {},
   "source": [
    "### Reflection 3: Why did you choose these features? How might they impact predictions or accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29ff9d",
   "metadata": {},
   "source": [
    "### Justification for Regression and Feature Selection\n",
    "\n",
    "The target variable, `SalePrice`, was chosen as it represents the continuous outcome we aim to predict, making this a regression problem. The input features were selected based on their potential correlation with `SalePrice`:\n",
    "\n",
    "- **GrLivArea** and **HouseAge** are numerical features likely to have a direct impact on housing prices (e.g., larger houses or newer houses may be more expensive).\n",
    "- **OverallQual**  is a ordinal categorical feature that captures quality and property type, which are strong indicators of value. It can also be used for Linear Regression\n",
    "- **Neighborhood** and **MSSubClassMapped** are nominal categorical features, which will not be used for the Linear Regression since they are more adaptable to categorical-friendly models like Decision Trees and Gradient Boosting.\n",
    "\n",
    "These features were chosen to balance interpretability and predictive power for the regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46f85f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adacca1f",
   "metadata": {},
   "source": [
    "## Section 4. Train a Model (Linear Regression)\n",
    "### 4.1 Split the data into training and test sets using train_test_split (or StratifiedShuffleSplit if class imbalance is an issue).\n",
    "### 4.2 Train model using Scikit-Learn model.fit() method\n",
    "### 4.3 Evalulate performance, for example:\n",
    "- Regression: R^2, MAE, RMSE (RMSE has been recently updated)\n",
    "- Classification: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n",
    "- Clustering: Inertia, Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessing pipeline to the cleaned data\n",
    "preprocessor.fit(data_cleaned)\n",
    "\n",
    "# Retrieve feature names for the transformed data\n",
    "numerical_feature_names = numerical_features\n",
    "onehot_feature_names = preprocessor.named_transformers_['onehot'].get_feature_names_out(categorical_features_onehot)\n",
    "ordinal_feature_names = categorical_features_ordinal\n",
    "\n",
    "# Combine all feature names\n",
    "all_lr_feature_names = list(numerical_feature_names) + list(ordinal_feature_names)\n",
    "\n",
    "# Display the feature names\n",
    "print(\"Transformed feature names:\")\n",
    "print(all_lr_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49654d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of feature names from the transformed data\n",
    "feature_names = all_lr_feature_names  # Use the variable containing all transformed feature names\n",
    "\n",
    "results = []  # Initialize an empty list to store results\n",
    "\n",
    "# Generate all possible combinations of features\n",
    "for r in range(1, len(feature_names) + 1):  # r is the number of features in the combination\n",
    "    for combination in combinations(feature_names, r):\n",
    "        # Define X and y\n",
    "        X_combination = data_transformed_df[list(combination)]  # Use the current combination of features\n",
    "        y = data_cleaned['SalePrice']  # Target variable\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_combination, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train the Linear Regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        \n",
    "        # Evaluate the model's performance\n",
    "        mse = mean_squared_error(y_val, y_val_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Features': combination,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ensure R2 column is numeric\n",
    "results_df['R2'] = pd.to_numeric(results_df['R2'], errors='coerce')\n",
    "\n",
    "# Sort results by R² score in descending order\n",
    "results_df = results_df.sort_values(by='R2', ascending=False)\n",
    "\n",
    "# Save all results to a CSV file\n",
    "results_df.to_csv('feature_combinations_results.csv', index=False)\n",
    "\n",
    "# Display the top 3 results\n",
    "print(\"Feature Combinations ordered by R^2 Score:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b9e15",
   "metadata": {},
   "source": [
    "### Performance Validation: Predicted Values vs. Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d73b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_val_pred, alpha=0.6, color='green')\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], color='red', linestyle='--')\n",
    "plt.title('Actual vs. Predicted SalePrice')\n",
    "plt.xlabel('Actual SalePrice')\n",
    "plt.ylabel('Predicted SalePrice')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ecf9d",
   "metadata": {},
   "source": [
    "### Cross-Validation to further check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c90eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_combination, y, cv=5, scoring='r2')  # 5-fold cross-validation\n",
    "print(f\"Cross-Validation R² Scores: {cv_scores}\")\n",
    "print(f\"Mean R² Score: {np.mean(cv_scores):.2f}\")\n",
    "print(f\"Standard Deviation of R² Scores: {np.std(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9da73",
   "metadata": {},
   "source": [
    "### Learning Curve Analysis to further check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ce0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curve data\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    model, X_combination, y, cv=5, scoring='r2', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training Score', color='blue')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_mean, label='Validation Score', color='orange')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='orange')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R² Score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8674a9",
   "metadata": {},
   "source": [
    "### Final Check: Correlation Matrix to check for Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72045946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = data_cleaned[['GrLivArea', 'HouseAge', 'OverallQual']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276380c",
   "metadata": {},
   "source": [
    "### Reflection 4: How well did the model perform? Any surprises in the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7495dc",
   "metadata": {},
   "source": [
    "# Model Evaluation Report\n",
    "\n",
    "## Top 3 Feature Combinations\n",
    "\n",
    "### 1. (GrLivArea, HouseAge, OverallQual)\n",
    "- **MAE**: 24,857.34  \n",
    "- **RMSE**: 33,146.29  \n",
    "- **R²**: 0.7819  \n",
    "- _This combination performed the best, achieving the lowest error metrics (MAE and RMSE) and the highest R² score. Including all three features captures the most variance in SalePrice._\n",
    "\n",
    "### 2. (GrLivArea, OverallQual)\n",
    "- **MAE**: 26,876.90  \n",
    "- **RMSE**: 36,680.20  \n",
    "- **R²**: 0.7330  \n",
    "- _Excluding HouseAge slightly reduces performance, indicating that HouseAge contributes meaningful information to the model._\n",
    "\n",
    "### 3. (GrLivArea, HouseAge)\n",
    "- **MAE**: 28,549.96  \n",
    "- **RMSE**: 39,241.90  \n",
    "- **R²**: 0.6944  \n",
    "- _Excluding OverallQual results in the weakest performance, suggesting that OverallQual is a critical predictor of SalePrice._\n",
    "\n",
    "---\n",
    "\n",
    "## Cross-Validation Results\n",
    "\n",
    "- **R² Scores**: [0.7852, 0.7966, 0.7678, 0.7681, 0.7405]  \n",
    "- **Mean R²**: 0.77  \n",
    "- **Standard Deviation**: 0.02  \n",
    "\n",
    "_The cross-validation results show consistent performance across folds, indicating that the model generalizes well to unseen data._\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Curve Insights\n",
    "\n",
    "- **Training Score**: Starts high and decreases as the training set size increases, indicating learning progression.\n",
    "- **Validation Score**: Remains stable, suggesting good generalization and no overfitting.\n",
    "- **Gap Between Scores**: Indicates potential for improvement, possibly by adding more features or using a more complex model.\n",
    "\n",
    "---\n",
    "\n",
    "## Correlation Matrix Insights\n",
    "\n",
    "- **GrLivArea and OverallQual**: Moderate positive correlation (0.57), suggesting both are strong predictors.\n",
    "- **HouseAge**: Weaker correlation with other features, but its inclusion still improves model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The best-performing model achieved an **R² of 0.7819** and a **low RMSE of 33,146.29**.\n",
    "- **Top Feature Set**: GrLivArea, HouseAge, and OverallQual.\n",
    "- The model shows **strong generalization** and is **not overfitting**, as supported by cross-validation and learning curve analysis.\n",
    "- _To further improve performance, consider exploring additional features or more advanced models such as Random Forest or Gradient Boosting._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b69b1",
   "metadata": {},
   "source": [
    "## Section 5. Improve the Model or Try Alternates (Implement Pipelines)\n",
    "### 5.1 Implement Pipeline 1: Imputer → StandardScaler → Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "numerical_features = ['GrLivArea', 'HouseAge']\n",
    "categorical_features_ordinal = ['OverallQual']\n",
    "\n",
    "# Define transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())                 # Scale numerical features\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('ordinal', OrdinalEncoder())                          # Ordinal encode categorical features\n",
    "])\n",
    "\n",
    "# Combine transformations using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('ord', ordinal_transformer, categorical_features_ordinal)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the full pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Preprocessing step\n",
    "    ('model', LinearRegression())    # Linear Regression model\n",
    "])\n",
    "\n",
    "# Define X and y\n",
    "X = data_cleaned[['GrLivArea', 'HouseAge', 'OverallQual']]\n",
    "y = data_cleaned['SalePrice']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ad27b",
   "metadata": {},
   "source": [
    "### 5.2 Implement Pipeline 2: Imputer → Polynomial Features (degree=3) → StandardScaler → Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "numerical_features = ['GrLivArea', 'HouseAge']\n",
    "categorical_features_ordinal = ['OverallQual']\n",
    "\n",
    "# Define transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),  # Generate polynomial features\n",
    "    ('scaler', StandardScaler())  # Scale numerical features\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('ordinal', OrdinalEncoder())  # Ordinal encode categorical features\n",
    "])\n",
    "\n",
    "# Combine transformations using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('ord', ordinal_transformer, categorical_features_ordinal)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the full pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Preprocessing step\n",
    "    ('model', LinearRegression())  # Linear Regression model\n",
    "])\n",
    "\n",
    "# Define X and y\n",
    "X = data_cleaned[['GrLivArea', 'HouseAge', 'OverallQual']]\n",
    "y = data_cleaned['SalePrice']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93171ab",
   "metadata": {},
   "source": [
    "### 5.3 Compare performance of all models across the same performance metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbad6ed",
   "metadata": {},
   "source": [
    "# Model Performance Comparison\n",
    "\n",
    "## Original Analysis\n",
    "- **Best Model**: (GrLivArea, HouseAge, OverallQual)  \n",
    "- **RMSE**: 33,146.29  \n",
    "- **R²**: 0.7819\n",
    "\n",
    "## Pipeline 1\n",
    "- **RMSE**: 33,146.29  \n",
    "- **R²**: 0.78  \n",
    "- _This pipeline performed similarly to the original analysis. It used imputation and scaling but did not introduce additional complexity._\n",
    "\n",
    "## Pipeline 2\n",
    "- **RMSE**: 31,613.86  \n",
    "- **R²**: 0.80  \n",
    "- _This pipeline outperformed both the original analysis and Pipeline 1, likely due to the inclusion of polynomial features (degree 3), which captured non-linear relationships in the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d9411",
   "metadata": {},
   "source": [
    "### Reflection 5: Which models performed better? How does scaling impact results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e836c93",
   "metadata": {},
   "source": [
    "## Insights on Scaling\n",
    "\n",
    "### Scaling Impact\n",
    "- Scaling ensures that numerical features are on the same scale.\n",
    "- This is especially important when:\n",
    "  - Using polynomial features\n",
    "  - Employing models sensitive to feature magnitudes (e.g., Linear Regression)\n",
    "- In **Pipeline 2**, scaling likely enhanced the model's ability to handle polynomial features effectively, leading to improved performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- **Best Model**: **Pipeline 2**  \n",
    "  - **RMSE**: 31,613.86  \n",
    "  - **R²**: 0.80  \n",
    "- **Key Takeaway**:  \n",
    "  Adding polynomial features and applying scaling improved the model's capacity to capture complex, non-linear relationships—resulting in better predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4808984",
   "metadata": {},
   "source": [
    "## Section 6. Final Thoughts & Insights\n",
    "### 6.1 Summarize findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f61c5",
   "metadata": {},
   "source": [
    "This Regression analysis attempted to predict housing prices from the train.csv dataset by utilizing three specific attributes: GrLivArea ( Above ground living area sqft), HouseAge (current year 2025 - yearbuilt), and OverallQual (Rating of the overall material and finish of the house).\n",
    "\n",
    "These attributes were chosen because they seemed to have strong potential for correlation with SalePrice, had no Null values in the dataset, and did not overlap significantly based on a correlation matrix.\n",
    "\n",
    "I tried applying the linear regression model with various combinations of the three attributes using a for loop method to iterate through all possible combinations (GrLivArea only, GrLivArea + HouseAge, all 3, etc.). I found that the combination with the highest R^2 score used all 3 attributes as the input features to predict the target variable SalePrice. This method yielded R^2 = 0.78 and RMSE = 33146, which means my model explained about 78% of the variablity in SalePrice based on the test data.\n",
    "\n",
    "Then I updated the pipeline to impute missing values with the mean for numerical features and the most common value for categorical features (although this yielded no changes since my train and test samples had 0 missing values).\n",
    "\n",
    "Finally, I added polynomial features with degree 3, which helped capture some of the non-linear relationships within the data. This strengthened the model to R^2 = 0.80 and RMSE = 31614. This can be considered a very good predicting model in the context of housing market analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4facb2",
   "metadata": {},
   "source": [
    "### 6.2 Discuss challenges faced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19496061",
   "metadata": {},
   "source": [
    "One challenge was feature choice, since there are a total of 81 possible features (columns) in the dataset. The possibilities are nearly endless for the different kinds of analysis that can be performed. I decided to stick with the 3 features GrLivArea, HouseAge, and OverallQual because they seem to be obviously logical considerations when house shopping. I also explored a few other categorical nominal features GarageType, Neighborhood, and MSSubClass as they would be my first choices in Classification analysis through the Decision Tree, Random Forest, Gradient Boosting, or other models.\n",
    "\n",
    "Another challenge was working with the YearBuilt attribute. I decided to feature engineer it into HouseAge to make it easier to interpret (as the HouseAge increases, the house is older. It also grounds the variable to 0, whereas keeping YearBuilt would mean that the starting value would be in the 1880s.)\n",
    "\n",
    "I also had the challenge of deciding how to handle outliers. There were clearly-visible outlier values within the GrLivArea input feature, as well as the SalePrice target variable. Ultimately, I decided to filter out these outlier values from those variables since they would unfairly skew the linear regression in a way that would be unrepresentative of the majority of the dataset. With that in mind, it should be stated that my analysis was configured for housing prices under $500k, so it may struggle to accurately predict very expensive housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502addc",
   "metadata": {},
   "source": [
    "### 6.3 If you had more time, what would you try next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea284d",
   "metadata": {},
   "source": [
    "I would run Classification analysis with Decision Trees, Random Forest, and Gradient Boosting on the categorical variables I ran EDA on but didn't include in the linear regression.\n",
    "\n",
    "I would also try running a neural network analysis by incorporating many of the features simultaneously. Since there are so many features (81) that could help predict housing prices, using a more complex model might yield interesting results. The more tools you have available, after all, means that there is plenty of fine-tuning that can be done. You might as well use everything at your disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f0477",
   "metadata": {},
   "source": [
    "### Reflection 6: What did you learn from this project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295bb0e",
   "metadata": {},
   "source": [
    "If I had to choose one takeaway, it would be the usefulness of pipelines. It clicked for me that they are similar to writing functions in Python; a data pipeline can be used again and again as new data comes in, so that we can prepare it for analysis. I decided to write a separate python script preprocessing_pipeline.py that can be imported and called in this project and future projects that might analyze this housing data.\n",
    "\n",
    "This course has been one of the most challenging and yet rewarding experiences of the program. I have an immense sense of accomplishment having worked through this analysis. I'm also breeming with other possibilities for analyzing this dataset with different models, so I am sure this is a project I will come back to throughout my data career. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
